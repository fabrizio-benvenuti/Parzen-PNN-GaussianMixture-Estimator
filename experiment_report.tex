<<<<<<< HEAD
% filepath: /home/fabrizio-benvenuti/git/Parzen-PNN-GaussianMixture-Estimator/experiment_report.tex
\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{tabularx}

=======
\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
>>>>>>> aef20ee ([ESTIMATOR] end of the estimation process)
\begin{document}

\title{Parzen-PNN Gaussian Mixture Estimator: Experiment Report}
\author{Fabrizio Benvenuti}
\date{\today}
\maketitle

\begin{abstract}
<<<<<<< HEAD
    \begin{center}
        This report will describe the results and performance differences between
        Parzen Window and Parzen Neural Network (PNN) estimation methods.
        They will be benchmarked against two-dimensional
        Probability Density Functions formed by a Mixture of Gaussians;
        while varying the cardinality of the extracted point set, the architecture
        of the neural network, and the hyperparameters of both estimation methods.
    \end{center}
\end{abstract}

\section{Introduction}
\subsection{Selected PDF's overview}
    The project consists of estimating a previously selected two-dimensional PDF formed by a mixture of an odd number of Gaussians [1,3,5], using a finite number of sample points from them.\newline
    The selection process was carried out choosing each Gaussian's weights and statistical parameters (mean and variance)
    to avoid PDFs with either excessively overlapping peaks or ones that are too distant from each other. \newline
    This was done to also check wether high and low variance parts of the PDF were being estimated correctly.
\subsection{Sampling Method}
    The sampling is done by extracting a set of points from the PDF, normalizing the probability of choosing a gaussian based on its weight in the mixture. \newline
    The extraction process was implemented like so:
    \begin{itemize}
        \item Use a weighted random choice to select a Gaussian from the mixture.
        \item Extract a point from the selected Gaussian
        \item Compute the PDF value at that point by summing the weighted contributions of all Gaussians in the mixture.
    \end{itemize}

\section{Estimation Methods}
The PDF will then be estimated non-parametrically using the Parzen Window and the Parzen Neural Network, as will be described in the later sections.\newline
This Methods still fall into the supervised learning category, as they both require a training set of samples. \newline
In this section we will briefly describe the theoretical background of both methods.
\subsection{Parzen Window Estimation}
Given a dataset of independent and identically distributed (i.i.d.) samples $\underline{X_1}, \underline{X_2}, \ldots, \underline{X_n}$ drawn from an unknown distribution with density function $p(\overline{x})$, we estimate $p(\overline{x})$ using Parzen Windows as:
\begin{equation}
    p_n(\overline{x_0}) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{V_n} \phi \left( \frac{\underline{x_0} - \underline{x_i}}{h_n} \right),
\end{equation}
where:
\begin{itemize}
    \item $n$ is the cardinality of the training set.
    \item $h_n$ is the window width (bandwidth).
    \item $V_n$ is the volume of the window.
    \item $\phi(\cdot)$ is a kernel function, commonly chosen as the Gaussian:
    \begin{equation}
        \phi(u) = N(\underline{u};\underline{0};I)
    \end{equation}
    this choice between all simmetric, always positive,\newline
    and unitary volume functions,was made because of the target pdf's nature
\end{itemize}
=======
    \centering
    This document aims at describing how different non parametric Estimation methods () perform.
    It details the underlying theory, including Parzen window estimation, Probabilistic Neural Networks, and Gaussian Mixture Models, describes the experimental setup, and discusses the obtained results.
\end{abstract}

\section{Introduction}
The experiment focused on evaluating a hybrid approach combining Parzen window techniques with Probabilistic Neural Networks (PNN) and Gaussian Mixture Models (GMM). The goal was to assess performance differences across various network architectures and activation functions.

\section{Theory}
\subsection{Parzen Window Estimation}
\begin{abstract}
    \centering
    Parzen Windows is a non-parametric method used for density estimation, while Parzen Neural Networks (PNNs) leverage this method for classification. This document explores the theoretical foundation, mathematical formulation, and practical applications of both techniques in machine learning.
\end{abstract}

\section{Introduction}
Many machine learning tasks require estimating probability density functions (PDFs) when the underlying distribution is unknown. Parzen Windows provide a kernel-based approach for density estimation, which serves as the foundation for Parzen Neural Networks. PNNs use a probabilistic approach to classification and offer advantages in accuracy and robustness while being computationally expensive. 

\section{Parzen Window Estimation}
Given a dataset of independent and identically distributed (i.i.d.) samples $X_1, X_2, ..., X_n$ drawn from an unknown distribution with density function $f(x)$, we estimate $f(x)$ using Parzen Windows as:
\begin{equation}
    \hat{f}(x) = \frac{1}{n h^d} \sum_{i=1}^{n} K \left( \frac{x - X_i}{h} \right),
\end{equation}
where:
\begin{itemize}
    \item $n$ is the number of training samples.
    \item $h$ is the window width (bandwidth).
    \item $d$ is the dimensionality of the data.
    \item $K(\cdot)$ is a kernel function, commonly chosen as the Gaussian:
    \begin{equation}
        K(u) = \frac{1}{(2\pi)^{d/2}} e^{-\frac{1}{2} u^T u}.
    \end{equation}
\end{itemize}

>>>>>>> aef20ee ([ESTIMATOR] end of the estimation process)
The choice of $h$ affects the estimatorâ€™s properties:
\begin{itemize}
    \item Large $h$ oversmooths the density estimate (high bias, low variance).
    \item Small $h$ leads to high variance, making the estimate sensitive to noise.
\end{itemize}

\section{Parzen Neural Networks (PNNs)}
<<<<<<< HEAD
A Parzen Neural Network consists of four layers:
\begin{enumerate}
    \item \textbf{Input Layer}: Receives the feature vector $x$.
    \item \textbf{Pattern Layer}: Each neuron represents a training sample and applies a kernel function:
    \begin{equation}
        \phi_i(x) = e^{-\frac{||x - X_i||^2}{2\sigma^2}}.
    \end{equation}
    \item \textbf{Summation Layer}: Computes class-wise probability estimates:
    \begin{equation}
        S_k(x) = \sum_{i \in C_k} \phi_i(x).
    \end{equation}
    \item \textbf{Decision Layer}: Selects the class with the highest probability:
    \begin{equation}
        C^* = \arg \max_k S_k(x).
    \end{equation}
\end{enumerate}

 \subsection{Estimation performance Calculation}
    The performance of each method will be evaluated based on several metrics, including:
    \begin{itemize}
        \item Mean Squared Error (MSE)
        \item Root Mean Squared Error (RMSE)
        \item Maximum Absolute Error
        \item Mean Absolute Error (MAE)
    \end{itemize}
        \subsubsection{Parzen Window estimation}
            The Parzen Window method estimates the probability density function (PDF) by placing a kernel at each sampled data point and averaging their contributions over a . The window size (bandwidth) controls the smoothness of the estimate, with smaller windows capturing more detail but increasing variance. In the experiments, the method was evaluated by varying both the number of sampled points and the window size, and the estimation error was measured against the true Gaussian mixture PDF. This approach provides a flexible, non-parametric way to approximate
            complex distributions from finite samples.
        \subsubsection{Parzen Neural Network estimation}
            The latter will consist of 1 or 2 hidden layers with a sigmoid activation function.
            and the output layer will use either ReLU or sigmoid functions with variable amplitude.
\section{Comparison with Other Models}
% Fix: Use X columns for automatic wrapping, reduce font size, and improve centering

\renewcommand{\arraystretch}{1.2} % Optional: better row spacing

% Use footnotesize for a more compact table

\begin{table}[h]
    \begin{center}
        \footnotesize
        \begin{tabularx}{\textwidth}{|X|X|X|X|X|}
            \hline
            \textbf{Model} & \textbf{Training Time} & \textbf{Classification Time} & \textbf{Memory Usage} & \textbf{Robustness} \\
            \hline
            Parzen\newline Windows & Slow & Slow & High & Good \\
            Parzen Neural Networks & Fast & Slow & Very High & Excellent \\
            k-NN Classifier & Fast & Slow & High & Good \\
            SVMs & Slow & Fast & Low & Excellent \\
            Deep Neural Networks & Very Slow & Fast & Medium-High & Good \\
            \hline
        \end{tabularx}
    \end{center}
    \caption{Comparison of Parzen-based models with other ML techniques.}
\end{table}



\subsection{Probabilistic Neural Networks (PNN)}
A Probabilistic Neural Network is designed for classification tasks by modeling the probability density functions of different classes. Its formulation supports rapid training and high noise tolerance.

\subsection{Gaussian Mixture Models (GMM)}
Gaussian Mixture Models assume that all data points are generated from a mixture of several Gaussian distributions. This approach is effective for clustering and modeling complex data distributions.
=======
A Parzen Neural Network is any feed-forward \textbf{ANN},\newline
equipped with a supervised learning algorithm,\newline
trained as follows:\newline
\textbf{Algorithm:} train a feedforward ANN to learn the $p(x)$\newline
from an unsupervised dataset $\tau$\newline
\textbf{inputs:}
\begin{itemize}
    \item \textbf{ANN} architecture and hyperparameters
    \item training dataset: $\tau = \{x_1, x_2, ..., x_n\}$
    \item initial edge: $h_1$
\end{itemize}
\textbf{output:} the estimate of $p(x)$: $\hat{p}(.)$\newline
\newline
\textbf{procedure:}

>>>>>>> aef20ee ([ESTIMATOR] end of the estimation process)

\subsection{Integration}
In this experiment, combining Parzen window estimation with PNN and GMM leverages the advantages of both non-parametric and probabilistic approaches, providing robust performance for density estimation and classification tasks.

\section{Experimental Setup}
Different configurations were tested:
\begin{itemize}
    \item Architectures with varying layers (e.g., 20, 10, 50, etc.).
    \item Activation functions including Tanh, Sigmoid, and LeakyReLU.
    \item Variation in parameters such as the number of kernels (nk) and bandwidth (bw).
\end{itemize}
The performance was monitored by analyzing the loss function over several training epochs.

\section{Results}
The logs indicate that:
\begin{itemize}
    \item The configuration with 20 and 10 layers using Tanh activation showed a consistent decrease in loss, achieving values below 0.002 by epoch 400.
    \item The 50-layer configuration with Sigmoid activation maintained a higher loss value, indicating limited improvement.
    \item The setup with 30-20-10 layers using LeakyReLU activation displayed rapid loss decrease, suggesting effective learning.
\end{itemize}

\section{Conclusions}
The experiment confirms that the network architecture and the activation function significantly affect convergence and performance. The integration of Parzen window estimation with PNN and GMM is promising for density estimation problems. Further research could optimize these configurations for broader applications.

<<<<<<< HEAD
\end{document}This document explains the experiment carried out with the Parzen-
PNN Gaussian Mixture Estimator. It details the underlying theory, in-
cluding Parzen window estimation, Probabilistic Neural Networks, and
Gaussian Mixture Models, describes the experimental setup, and discusses
the obtained results.
=======
\end{document}
>>>>>>> aef20ee ([ESTIMATOR] end of the estimation process)
