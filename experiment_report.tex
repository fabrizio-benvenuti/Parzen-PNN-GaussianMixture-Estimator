% filepath: /home/fabrizio-benvenuti/git/Parzen-PNN-GaussianMixture-Estimator/experiment_report.tex
\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[ruled,vlined]{algorithm2e}
\RestyleAlgo{ruled}
\usepackage{tabularx}

\begin{document}

\title{Parzen-PNN Gaussian Mixture Estimator: Experiment Report}
\author{Fabrizio Benvenuti}
\date{\today}
\maketitle

\begin{abstract}
    \begin{center}
        This report will describe the results and performance differences between
        Parzen Window and Parzen Neural Network (PNN) estimation methods.
        They will be benchmarked against two-dimensional
        Probability Density Functions formed by a Mixture of Gaussians;
        while varying the cardinality of the extracted point set, the architecture
        of the neural network (kernel parameterization), and the hyperparameters of both estimation methods.
    \end{center}
\end{abstract}

\section{Introduction}
\subsection{Project Objective}
The objective of the project is to study \emph{non-parametric} density estimation in $\mathbb{R}^2$ when the
ground truth is a two-dimensional Gaussian Mixture Model (GMM). Given an unlabeled sample set
$\{x_i\}_{i=1}^n\subset\mathbb{R}^2$ drawn i.i.d. from an unknown target density $p(x)$, the goal is to construct an
estimate $\hat p(x)$ that approximates the true density on a fixed evaluation domain.

\subsection{Ground Truth and Sampling}
The estimators are benchmarked against three synthetic target PDFs in $\mathbb{R}^2$, each given by a mixture
of Gaussians with an odd number of components $M\in\{1,3,5\}$:
\begin{equation}
    p(x)=\sum_{m=1}^{M}\pi_m\,\mathcal{N}(x;\mu_m,\Sigma_m),\qquad \pi_m\ge 0,\ \sum_{m=1}^{M}\pi_m = 1.
\end{equation}
The unlabeled dataset is generated using \textbf{ancestral sampling} from the mixture:
\begin{equation}
    J \sim \mathrm{Categorical}(\pi_1,\ldots,\pi_M),
    \qquad X\mid(J=m) \sim \mathcal{N}(\mu_m,\Sigma_m).
\end{equation}
In practice, an index $J$ is sampled according to the mixture weights and then $X$ is sampled from the chosen
Gaussian component. Only the sample locations $\{x_i\}_{i=1}^{n}$ are used to train the estimators.

\subsection{Methods Compared}
Two non-parametric density estimators are compared:
\begin{itemize}
    \item \textbf{Parzen Window (PW / KDE):} a kernel density estimator that averages local kernel
    contributions centered at the samples.
    \item \textbf{Parzen Neural Network (PNN):} a neural surrogate trained by regression on Parzen-style
    targets computed from the samples.
\end{itemize}

\section{Theoretical Foundations of Density Estimation}
\subsection{The Parzen Window Method (KDE)}
Let $Y=\{x_1,\ldots,x_n\}\subset\mathbb{R}^d$ be i.i.d.\ samples drawn from an unknown density $p(x)$.
For a query point $x_0$, consider a region $R_n(x_0)\subset\mathbb{R}^d$ with volume $V_n$ and let $k_n$
be the number of samples falling in $R_n$. A generic non-parametric density estimator is
\begin{equation}
    p_n(x_0) = \frac{k_n/n}{V_n}.
    \label{eq:counting-estimator}
\end{equation}
Since $k_n$ depends on the random sample, $p_n(x_0)$ is itself a random variable. The estimator is
consistent in probability if and only if
\begin{equation}
    \lim_{n\to\infty} V_n = 0, \qquad
    \lim_{n\to\infty} k_n = \infty, \qquad
    \lim_{n\to\infty} \frac{k_n}{n} = 0.
    \label{eq:consistency-conditions}
\end{equation}
Two complementary constructions satisfy these conditions: fixing $V_n$ (Parzen Window) or fixing $k_n$
(k-nearest neighbors).

In modern form, Parzen Window estimation is presented directly as \emph{kernel density estimation} (KDE):
\begin{equation}
    \hat p_n(x) = \frac{1}{n\,h_n^d}\sum_{i=1}^{n}K\!\left(\frac{x-x_i}{h_n}\right),
    \qquad K(u)\ge 0,\ \int_{\mathbb{R}^d}K(u)\,du=1.
    \label{eq:kde-general}
\end{equation}
The historical rectangular window corresponds to choosing $K$ as an indicator over a hypercube; however,
the experiments considered here use only smooth kernels.

For $d=2$ and an isotropic Gaussian kernel, Eq.~\eqref{eq:kde-general} becomes
\begin{equation}
    \hat p_n(x)=\frac{1}{n}\sum_{i=1}^{n}\mathcal{N}(x; x_i, h_n^2 I_2)
    = \frac{1}{n\,(2\pi h_n^2)}\sum_{i=1}^{n}\exp\!\left(-\frac{\|x-x_i\|^2}{2h_n^2}\right).
    \label{eq:kde-gaussian-2d}
\end{equation}

\subsection{Kernel Choice}
The counting derivation above is typically introduced with a \emph{rectangular} (hard) window, which
produces discontinuous estimates and visually blocky density surfaces on a grid. Here a
    extbf{Gaussian kernel} is used to obtain smooth, infinitely differentiable ($C^\infty$) density estimates.
Compared to a rectangular kernel, Gaussian KDE avoids discontinuities at window boundaries and yields
better numerical behavior when gradients are involved (e.g., when KDE targets are distilled into a neural
surrogate). Importantly, the choice of kernel is a numerical/modelling choice and does \emph{not} assume
any knowledge of the parametric family of the true density.

\subsection{Bandwidth Parameterization}
A distinction is made between a user-chosen \emph{base} bandwidth $h_1$ and the \emph{effective} bandwidth $h_n$ used in
the kernels, using the rule
\begin{equation}
    h_n = \frac{h_1}{\sqrt{n-1}},\qquad V_n=h_n^d.
    \label{eq:hn-def}
\end{equation}
The subscript $n$ indicates explicit dependence on the sample size. In standard KDE theory, consistency
for a smooth kernel in $d$ dimensions is typically obtained by choosing a sequence $h_n$ such that
\begin{equation}
    h_n\to 0,\qquad n h_n^d \to \infty \qquad (n\to\infty),
    \label{eq:kde-consistency}
\end{equation}
which balances vanishing bias (from $h_n\to 0$) and vanishing variance (from $n h_n^d\to\infty$). A common
parametric family is $h_n = h_1 n^{-\alpha}$ with $0<\alpha<\tfrac{1}{d}$.

The specific decay in Eq.~\eqref{eq:hn-def} corresponds to the borderline case $\alpha=\tfrac{1}{2}$ in
$d=2$, for which $n h_n^2$ does \emph{not} diverge asymptotically. Therefore, the choice in
Eq.~\eqref{eq:hn-def} is treated here as a \emph{finite-sample heuristic} that shrinks the bandwidth
aggressively as more samples are available. This makes the bias--variance trade-off explicit: if $h_n$
decreases too slowly the estimator remains biased (oversmoothing), while if it decreases too fast the
estimate becomes noisy (undersmoothing) because too few samples contribute effectively to each kernel.
In the experiments, sweeps are reported in terms of $h_1$, while kernel computations use $h_n$.

\section{Parzen Neural Networks (PNN)}
\subsection{Architecture and Training Algorithm}
A Parzen Neural Network is an artificial neural network trained to regress non-parametric Parzen Window
density estimates. Given samples $\tau=\{x_1,\ldots,x_n\}\subset\mathbb{R}^d$, the network learns an
approximation of the Parzen estimator and is then used as a continuous surrogate of the probability
density.
\newpage
\begin{algorithm}
\caption{Train Parzen Neural Network}\label{alg:train-pnn}
\KwData{samples $\tau=\{x_1,\ldots,x_n\}$, bandwidth $h_1$, kernel $\phi$, ANN architecture and optimizer hyperparameters}
\KwResult{Trained ANN parameters; unnormalized density surrogate $\hat p_\theta(\cdot)$}
Use effective bandwidth $h_n$ (Eq.~\eqref{eq:hn-def}) and $V_n=h_n^d$\;
\For{$i=1$ \KwTo $n$}{
    $\tau_i \leftarrow \tau\setminus\{x_i\}$\;
    $y_i \leftarrow \dfrac{1}{n-1}\sum_{x\in\tau_i}\dfrac{1}{V_n}\,
    \phi\!\left(\dfrac{x_i-x}{h_n}\right)$\;
}
$S \leftarrow \{(x_i,y_i)\}_{i=1}^n$\;
Sample $k$ auxiliary points $u_j\sim\mathrm{Unif}(D)$\;
\For{$j=1$ \KwTo $k$}{
    $\tilde y_j \leftarrow \dfrac{1}{n}\sum_{i=1}^{n}\dfrac{1}{V_n}\,\phi\!\left(\dfrac{u_j-x_i}{h_n}\right)$\;
}
$S\leftarrow S\cup\{(u_j,\tilde y_j)\}_{j=1}^{k}$\;
Train ANN by regression on $S$ (e.g.\ MSE loss)\;
$\hat p_\theta(\cdot)\leftarrow$ function computed by the trained ANN\;
\end{algorithm}

\subsection{Output Constraints and Non-Negativity}
A PNN is trained to regress density targets and is used as a non-negative density surrogate
$\hat p:\mathbb{R}^d\to\mathbb{R}_+$. Non-negativity is enforced by choosing the output as
\begin{equation}
    \hat p(x) = g(z(x)), \qquad g:\mathbb{R}\to\mathbb{R}_+,
\end{equation}
so that
\begin{equation}
    \hat p(x)\ge 0,\qquad \forall x\in\mathbb{R}^d.
\end{equation}
Typical choices are ReLU or a scaled sigmoid.

Three practical ways of enforcing non-negativity are considered:
\begin{itemize}
    \item \textbf{ReLU output:} $\hat p(x)=\max(0,z(x))$.
    \item \textbf{Scaled sigmoid:} $\hat p(x)=A\,\sigma(z(x))$ with $A>0$.
    \item \textbf{Log-parameterized mode:} the network outputs an unconstrained scalar $s_\theta(x)\in\mathbb{R}$
    interpreted as a log-density score.
\end{itemize}
In log-density mode, define
\begin{equation}
    \hat p_\theta(x) = \exp(s_\theta(x)).
\end{equation}
Training becomes regression on log-targets:
\begin{equation}
    \min_\theta\ \frac{1}{n}\sum_{i=1}^{n}\big(s_\theta(x_i)-\log(y_i+\varepsilon)\big)^2
    \equiv
    \min_\theta\ \frac{1}{n}\sum_{i=1}^{n}\big(\log \hat p_\theta(x_i)-\log(y_i+\varepsilon)\big)^2.
    \label{eq:log-density-loss}
\end{equation}
This transformation compresses the dynamic range of density values and turns multiplicative errors in
density space into additive errors in log-space, which is typically more numerically stable.

For the scaled-sigmoid output, the scale $A$ is chosen to match the typical magnitude of the Parzen
targets. A simple and effective heuristic is to set
\begin{equation}
    A = c\,\max_i y_i,\qquad c>1,
\end{equation}
so that the network is not artificially prevented from exceeding the maximum observed target during
approximation. In the reported experiments, $c=1.5$ is used as a conservative ``headroom'' factor.

The targets $y_i$ are constructed with a leave-one-out Parzen estimator, which avoids the self-kernel
contribution and yields an (asymptotically) unbiased estimate at sample locations:
\begin{equation}
    y_i=\frac{1}{n-1}\sum_{x\in\tau\setminus\{x_i\}}\frac{1}{V_n}
    \phi\!\left(\frac{x_i-x}{h_n}\right).
\end{equation}
Since targets are available only at the sample locations, the behavior between and outside samples is
determined by the inductive bias of the network architecture.

\subsection{Normalization over a Finite Domain}
A PNN does not enforce normalization:
\begin{equation}
    \int_{\mathbb{R}^d}\hat p(x)\,dx \neq 1 \quad \text{in general}.
\end{equation}
Therefore $\hat p$ is interpreted as an unnormalized density estimate. When a proper pdf is required,
normalization is performed on a fixed rectangle $D\subset\mathbb{R}^2$ discretized by a uniform grid
$\mathcal{G}=\{u_m\}_{m=1}^{M_D}$. Let $\Delta A=\Delta x\,\Delta y$ be the elementary cell area. The
normalizing constant is approximated by the Riemann sum
\begin{equation}
    Z \approx \sum_{u_m\in\mathcal{G}} \hat p_\theta(u_m)\,\Delta A,
    \qquad
    \hat p_{\mathrm{norm}}(x) = \frac{\hat p_\theta(x)}{Z}.
    \label{eq:riemann-normalization}
\end{equation}
The accuracy of Eq.~\eqref{eq:riemann-normalization} depends on both grid resolution and the choice of
domain $D$. If $D$ is too small, non-negligible probability mass may lie outside $D$, biasing the
normalization and any likelihood-style metric computed on $D$.

\section{Advanced Regularization Techniques}
\subsection{PDF Support and Boundary Penalty}
Standard activations are non-local basis functions; therefore fitting $\hat p(x_i)\approx y_i$ does not
imply $\hat p(x)\to 0$ away from the data and may generate heavy tails. To encourage compact support,
boundary constraints can be introduced. Let $X\subset\mathbb{R}^d$ denote the chosen support rectangle
(typically the same domain used for evaluation). Let $\xi$ be the diameter of $X$ and set $\delta=\alpha\xi$ with small
$\alpha\in(0,1)$. Define
\begin{equation}
    B_\delta=\{x\in\mathbb{R}^d\mid \mathrm{dist}(x,X)<\delta\}, \qquad
    \overline{B}_\delta=B_\delta\setminus X,
\end{equation}
with $\mathrm{dist}(x,X)=\inf_{y\in X}\|x-y\|$. Sample $\tilde x_j\sim\mathrm{Unif}(\overline{B}_\delta)$ and
add zero-density labels $S_\delta=\{(\tilde x_j,0)\}$. Training on $S\cup S_\delta$ can be interpreted as
minimizing the empirical regularized objective
\begin{equation}
    \min_\theta\ \frac{1}{n}\sum_{i=1}^n(\hat p_\theta(x_i)-y_i)^2
    +\lambda\,\frac{1}{k}\sum_{j=1}^k \hat p_\theta(\tilde x_j)^2,
\end{equation}
which penalizes probability mass near the boundary of $X$, stabilizes numerical normalization, and
mitigates spurious heavy tails.

\subsection{Internal Uniform Supervision}
The training set may be augmented with additional points sampled uniformly inside
the evaluation domain $D$. For each uniform point $u_j\sim\mathrm{Unif}(D)$ a Parzen/KDE target is computed
using the same kernel (with bandwidth $h_n$) and the pairs $\{(u_j, \hat p_{\mathrm{KDE}}(u_j))\}$ are added to
the regression dataset. This provides supervision away from sample locations and reduces extrapolation
artifacts when training only on pointwise leave-one-out targets.

\section{Experimental Setup and Model Selection}
\subsection{Dynamic Variables}
The experiments are organized as controlled sweeps over:
\begin{itemize}
    \item \textbf{Sample size $n$:} approximately 50 to 200 samples per Gaussian component.
    \item \textbf{PW bandwidth:} a grid of $h_1$ values spanning under- to over-smoothing.
    \item \textbf{PNN optimization:} learning-rate sweeps for Adam over a predefined grid; representative
    bandwidth values $h_1\in\{2,7,12,16\}$ are used for PNN training runs.
\end{itemize}

\subsection{Candidate Architectures}
MLPs with sigmoid hidden activations and non-negative outputs (ReLU or scaled sigmoid) are tested. The
architectures used in the sweeps include the following hidden-layer widths:
\begin{itemize}
    \item \textbf{[20]} and \textbf{[30,20]} with \textbf{output ReLU}.
    \item \textbf{[20]} and \textbf{[30,20]} with \textbf{output scaled-sigmoid} (with $A$ chosen
    automatically in the implementation).
\end{itemize}
This isolates the effect of capacity (width/depth) and output parameterization while keeping the target
generation mechanism fixed (leave-one-out Parzen targets).

From a modelling standpoint, increasing depth (e.g., moving from one to two hidden layers) can improve
representation of anisotropy and multimodality within a compact domain (pro), but may increase
overfitting risk and exacerbate spurious ``heavy tails'' outside high-density regions if not properly
regularized (con).

\subsection{Validation Criteria}
To select hyperparameters without using the ground truth, a held-out validation set
$\{x_j\}_{j=1}^{m}$ and compute the \textbf{validation negative log-likelihood (NLL)}:
\begin{equation}
    \mathrm{NLL}(\hat p;\{x_j\}_{j=1}^{m}) = -\frac{1}{m}\sum_{j=1}^{m}\log\big(\hat p(x_j)+\varepsilon\big).
\end{equation}
For PW/KDE, $\hat p$ is already normalized. For PNNs, normalization is performed on the finite domain $D$
(Section 3.3) before computing the NLL. This aligns with the principle of selecting the simplest model
that performs well on held-out data (an MDL-style viewpoint).

\section{Results Analysis and Discussion}
\subsection{Quantitative Performance}
Performance is evaluated on a fixed grid over a rectangle $D\subset\mathbb{R}^2$ that covers the target
mixtures. Accuracy is summarized via the $L_2$ error over the grid (a Riemann approximation of the
$L_2(D)$ norm):
\begin{equation}
    \|\hat p - p\|_{L^2(D)}^2 \approx \sum_{u_m\in\mathcal{G}}\big(\hat p(u_m)-p(u_m)\big)^2\,\Delta A.
\end{equation}
This allows direct comparison between PW and PNN as $n$ and $h_1$ vary.

Figure~\ref{fig:val-nll-sweeps} reports the data-only cross-validation criterion (validation NLL on held-out
points) across bandwidths. Because PNNs are normalized on a finite domain $D$ (Section 3.3), their NLL
depends on $D$; the comparison is still meaningful as long as the same $D$ is used consistently.

\begin{figure}[h]
\centering
\includegraphics[width=0.32\linewidth]{figures/validation_nll_bandwidth_sweep_mixture1.jpeg}
\includegraphics[width=0.32\linewidth]{figures/validation_nll_bandwidth_sweep_mixture2.jpeg}
\includegraphics[width=0.32\linewidth]{figures/validation_nll_bandwidth_sweep_mixture3.jpeg}
\caption{Validation NLL (data-only cross-validation) as a function of bandwidth $h_1$ for the three target
mixtures. Lower is better.}
\label{fig:val-nll-sweeps}
\end{figure}

For additional context (not usable for model selection), Table~\ref{tab:oracle-best-mse} summarizes the best
oracle grid MSE achieved among the saved training logs, where the MSE is computed against the known
mixture ground truth on the evaluation grid.

\begin{table}[h]
\centering
\begin{tabular}{l l r r}
\hline
Mixture & Best PNN configuration (logged) & $h_1$ & best grid MSE\\
\hline
1 & MLP [30,20], out ReLU (log-density training) & 2.0 & 2.68\,$\times 10^{-5}$\\
2 & MLP [30,20], out scaled-sigmoid (A=auto, log-density training) & 7.0 & 1.48\,$\times 10^{-5}$\\
3 & MLP [30,20], out ReLU (log-density training) & 12.0 & 8.84\,$\times 10^{-6}$\\
\hline
\end{tabular}
\caption{Best oracle grid MSE observed in the saved training logs for each mixture. These values use
ground truth and are included only for analysis and sanity-checking, not for selecting hyperparameters.}
\label{tab:oracle-best-mse}
\end{table}

\subsection{Qualitative Analysis (Overlays)}
In addition to scalar metrics, 3D surface plots are inspected comparing (i) the true mixture density and
(ii) the estimated density. Overlay visualizations help diagnose oversmoothing (large bandwidth),
spurious bumps (small bandwidth), and missing/merged modes.

\begin{figure}[h]
\centering
\includegraphics[width=0.32\linewidth]{figures/overlays_h1_12p00_mixture1.jpeg}
\includegraphics[width=0.32\linewidth]{figures/overlays_h1_12p00_mixture2.jpeg}
\includegraphics[width=0.32\linewidth]{figures/overlays_h1_12p00_mixture3.jpeg}
\caption{Overlay comparison (KDE vs true and PNN vs true) for a representative bandwidth ($h_1=12$),
showing the effect of architecture and output parameterization across mixtures.}
\label{fig:overlays-h1-12}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.32\linewidth]{figures/learning_results_bandwidth_sweep_mixture1.jpeg}
\includegraphics[width=0.32\linewidth]{figures/learning_results_bandwidth_sweep_mixture2.jpeg}
\includegraphics[width=0.32\linewidth]{figures/learning_results_bandwidth_sweep_mixture3.jpeg}
\caption{PNN learning behavior across bandwidths: training curves (top) and final oracle grid MSE vs $h_1$
(bottom), compared to KDE.}
\label{fig:learning-sweeps}
\end{figure}

\subsection{Complexity Analysis}
Let $T$ be the number of query points where the density is evaluated. Parzen Window / KDE requires
evaluating $n$ kernels per query point, giving inference complexity $\mathcal{O}(nT)$. A PNN has a
potentially expensive training phase (including leave-one-out target generation), but after training the
test-time cost does not scale with $n$: evaluating a fixed network on $T$ points is $\mathcal{O}(WT)$,
where $W$ is the number of network parameters (often summarized as linear in $T$ for fixed architecture).

The main training-time cost specific to PNNs is the construction of leave-one-out targets: for each of
the $n$ samples, $n-1$ kernel evaluations are required, yielding
\begin{equation}
    	ext{target generation cost} = \mathcal{O}(n^2).
\end{equation}
Training the network for $E$ epochs with $W$ parameters incurs an additional cost
$\mathcal{O}(n E W)$, so the total training complexity can be summarized as
\begin{equation}
    \mathcal{O}(n^2 + n E W).
\end{equation}

To support the asymptotic analysis with empirical evidence, Table~\ref{tab:runtime} reports measured
wall-clock runtimes on this machine for a single evaluation of $T=10^4$ query points.

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\hline
Method & $n$ & $T$ & time (s)\\
\hline
PW (Gaussian KDE) & 2000 & $10^4$ & 0.923\\
PNN forward (fixed $W$) & -- & $10^4$ & 7.66\,$\times 10^{-4}$\\
\hline
\end{tabular}
\caption{Measured inference time for PW vs PNN on $T=10^4$ points (single run).}
\label{tab:runtime}
\end{table}

\subsection{Effect of Boundary Penalty}
When boundary regularization (Section 4.1) is enabled, the model is discouraged from placing probability
mass near/outside the domain boundary. This reduces heavy tails, stabilizes finite-domain normalization,
and can improve validation NLL when an unregularized model assigns excessive mass to low-density regions.
The effect is assessed jointly via shape changes in overlay plots and validation NLL curves.

\begin{figure}[h]
\centering
\includegraphics[width=0.32\linewidth]{figures/boundary_penalty_comparison_mixture1.jpeg}
\includegraphics[width=0.32\linewidth]{figures/boundary_penalty_comparison_mixture2.jpeg}
\includegraphics[width=0.32\linewidth]{figures/boundary_penalty_comparison_mixture3.jpeg}
\caption{Boundary penalty comparison for each mixture, using the configuration selected by validation NLL.
For each mixture: KDE vs true (left), PNN with $\lambda=0$ (middle), PNN with $\lambda=10^{-2}$ (right).}
\label{fig:boundary-penalty-compare}
\end{figure}

\section{Conclusions}
The experiments compare a memory-based non-parametric estimator (PW/KDE) and a learned surrogate (PNN)
trained on Parzen-derived targets. The PNN can be viewed as a \emph{compressed} model that, with enough
capacity, can approximate complex multi-modal densities on a chosen compact domain.

This viewpoint is supported by the Universal Approximation Theorem: for a compact set $X\subset\mathbb{R}^d$
and any continuous target function $f\in C(X)$, a single-hidden-layer MLP with sigmoid activations can
approximate $f$ uniformly to arbitrary precision. In particular, for any $\varepsilon>0$ there exists a
network $f_\theta$ such that
\begin{equation}
    \sup_{x\in X}\,|f_\theta(x)-f(x)| < \varepsilon.
\end{equation}
Therefore, on a compact evaluation domain $D$, sufficiently wide sigmoid MLPs provide a principled function
class for approximating smooth density surfaces (after enforcing non-negativity and normalization).

Compared to PW/KDE, PNNs shift computation from test time to training time: once trained, inference is
efficient and does not grow with the number of samples. Overall performance depends on bandwidth choice,
architecture capacity, and regularization (boundary penalty and uniform supervision).

\begin{thebibliography}{1}
\bibitem{cybenko1989}
G.~Cybenko, ``Approximation by superpositions of a sigmoidal function,'' \emph{Mathematics of Control,
Signals and Systems}, vol.~2, no.~4, pp.~303--314, 1989.
\end{thebibliography}

\end{document}