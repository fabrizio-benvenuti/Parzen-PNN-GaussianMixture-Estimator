% filepath: /home/fabrizio-benvenuti/git/Parzen-PNN-GaussianMixture-Estimator/experiment_report.tex
\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage[margin=0.8in]{geometry}
\usepackage{graphicx}
\usepackage[ruled,vlined]{algorithm2e}
\RestyleAlgo{ruled}
\usepackage{tabularx}
\usepackage{float}
\usepackage{placeins}
\usepackage{parskip}

\begin{document}

\title{Parzen-PNN Gaussian Mixture Estimator: Experiment Report}
\author{Fabrizio Benvenuti}
\date{\today}
\maketitle

\begin{abstract}
    \begin{center}
        This report will describe the results and performance differences between
        Parzen Window and Parzen Neural Network (PNN) estimation methods.
        They will be benchmarked against two-dimensional
        Probability Density Functions formed by a Mixture of Gaussians;
        while varying the cardinality of the extracted point set, the architecture
        of the neural network (kernel parameterization), and the hyperparameters of both estimation methods.
    \end{center}
\end{abstract}

\section{Introduction}
\subsection{Project Objective}
The objective of the project is to study \emph{non-parametric} density estimation in $\mathbb{R}^2$ when the
ground truth is a two-dimensional Gaussian Mixture Model (GMM). Given an unlabeled sample set
$\{x_i\}_{i=1}^n\subset\mathbb{R}^2$ drawn i.i.d. from an unknown target density $p(x)$, the goal is to construct an
estimate $\hat p(x)$ that approximates the true density on a fixed evaluation domain.

\subsection{Ground Truth and Sampling}
The estimators are benchmarked against three synthetic target PDFs in $\mathbb{R}^2$, each given by a mixture
of Gaussians with an odd number of components $M\in\{1,3,5\}$:
\begin{equation}
    p(x)=\sum_{m=1}^{M}\pi_m\,\mathcal{N}(x;\mu_m,\Sigma_m),\qquad \pi_m\ge 0,\ \sum_{m=1}^{M}\pi_m = 1.
\end{equation}
The unlabeled dataset is generated using \textbf{ancestral sampling} from the mixture:
\begin{equation}
    J \sim \mathrm{Categorical}(\pi_1,\ldots,\pi_M),
    \qquad X\mid(J=m) \sim \mathcal{N}(\mu_m,\Sigma_m).
\end{equation}
In practice, an index $J$ is sampled according to the mixture weights and then $X$ is sampled from the chosen
Gaussian component. Only the sample locations $\{x_i\}_{i=1}^{n}$ are used to train the estimators.

\subsection{Methods Compared}
Two non-parametric density estimators are compared:
\begin{itemize}
    \item \textbf{Parzen Window (PW / KDE):} a kernel density estimator that averages local kernel
    contributions centered at the samples.
    \item \textbf{Parzen Neural Network (PNN):} a neural surrogate trained by regression on Parzen-style
    targets computed from the samples.
\end{itemize}

\section{Theoretical Foundations of Density Estimation}
\subsection{The Parzen Window Method (KDE)}
Let $Y=\{x_1,\ldots,x_n\}\subset\mathbb{R}^d$ be i.i.d.\ samples drawn from an unknown density $p(x)$.
For a query point $x_0$, consider a region $R_n(x_0)\subset\mathbb{R}^d$ with volume $V_n$ and let $k_n$
be the number of samples falling in $R_n$. A generic non-parametric density estimator is
\begin{equation}
    p_n(x_0) = \frac{k_n/n}{V_n}.
    \label{eq:counting-estimator}
\end{equation}
Since $k_n$ depends on the random sample, $p_n(x_0)$ is itself a random variable. The estimator is
consistent in probability if and only if
\begin{equation}
    \lim_{n\to\infty} V_n = 0, \qquad
    \lim_{n\to\infty} k_n = \infty, \qquad
    \lim_{n\to\infty} \frac{k_n}{n} = 0.
    \label{eq:consistency-conditions}
\end{equation}
Two complementary constructions satisfy these conditions: fixing $V_n$ (Parzen Window) or fixing $k_n$
(k-nearest neighbors).

In modern form, Parzen Window estimation is presented directly as \emph{kernel density estimation} (KDE):
\begin{equation}
    \hat p_n(x) = \frac{1}{n\,h_n^d}\sum_{i=1}^{n}K\!\left(\frac{x-x_i}{h_n}\right),
    \qquad K(u)\ge 0,\ \int_{\mathbb{R}^d}K(u)\,du=1.
    \label{eq:kde-general}
\end{equation}
The historical rectangular window corresponds to choosing $K$ as an indicator over a hypercube; however,
the experiments considered here use only smooth kernels.

For $d=2$ and an isotropic Gaussian kernel, Eq.~\eqref{eq:kde-general} becomes
\begin{equation}
    \hat p_n(x)=\frac{1}{n}\sum_{i=1}^{n}\mathcal{N}(x; x_i, h_n^2 I_2)
    = \frac{1}{n\,(2\pi h_n^2)}\sum_{i=1}^{n}\exp\!\left(-\frac{\|x-x_i\|^2}{2h_n^2}\right).
    \label{eq:kde-gaussian-2d}
\end{equation}

\subsection{Kernel Choice}
The counting derivation above is typically introduced with a \emph{rectangular} (hard) window, which
produces discontinuous estimates and visually blocky density surfaces on a grid. Here a
\textbf{Gaussian kernel} is used to obtain smooth, infinitely differentiable ($C^\infty$) density estimates.
Compared to a rectangular kernel, Gaussian KDE avoids discontinuities at window boundaries and yields
better numerical behavior when gradients are involved (e.g., when KDE targets are distilled into a neural
surrogate). Importantly, the choice of kernel is a numerical/modelling choice and does \emph{not} assume
any knowledge of the parametric family of the true density.

\subsection{Bandwidth Parameterization}
A distinction is made between a user-chosen \emph{base} bandwidth $h_1$ and the \emph{effective} bandwidth $h_n$ used in
the kernels, using the rule
\begin{equation}
    h_n = \frac{h_1}{\sqrt{n-1}},\qquad V_n=h_n^d.
    \label{eq:hn-def}
\end{equation}
The subscript $n$ indicates explicit dependence on the sample size. In standard KDE theory, consistency
for a smooth kernel in $d$ dimensions is typically obtained by choosing a sequence $h_n$ such that
\begin{equation}
    h_n\to 0,\qquad n h_n^d \to \infty \qquad (n\to\infty),
    \label{eq:kde-consistency}
\end{equation}
which balances vanishing bias (from $h_n\to 0$) and vanishing variance (from $n h_n^d\to\infty$). A common
parametric family is $h_n = h_1 n^{-\alpha}$ with $0<\alpha<\tfrac{1}{d}$.

The specific decay in Eq.~\eqref{eq:hn-def} corresponds to the borderline case $\alpha=\tfrac{1}{2}$ in
$d=2$, for which $n h_n^2$ does \emph{not} diverge asymptotically. Therefore, the choice in
Eq.~\eqref{eq:hn-def} is treated here as a \emph{finite-sample heuristic} that shrinks the bandwidth
aggressively as more samples are available. This makes the bias--variance trade-off explicit: if $h_n$
decreases too slowly the estimator remains biased (oversmoothing), while if it decreases too fast the
estimate becomes noisy (undersmoothing) because too few samples contribute effectively to each kernel.
In the experiments, sweeps are reported in terms of $h_1$, while kernel computations use $h_n$.

\section{Parzen Neural Networks (PNN)}
\subsection{Architecture and Training Algorithm}
A Parzen Neural Network is an artificial neural network trained to regress non-parametric Parzen Window
density estimates. Given samples $\tau=\{x_1,\ldots,x_n\}\subset\mathbb{R}^d$, the network learns an
approximation of the Parzen estimator and is then used as a continuous surrogate of the probability
density.
\newpage
\begin{algorithm}
\caption{Train Parzen Neural Network}\label{alg:train-pnn}
\KwData{samples $\tau=\{x_1,\ldots,x_n\}$, bandwidth $h_1$, kernel $\phi$, ANN architecture and optimizer hyperparameters}
\KwResult{Trained ANN parameters; unnormalized density surrogate $\hat p_\theta(\cdot)$}
Use effective bandwidth $h_n$ (Eq.~\eqref{eq:hn-def}) and $V_n=h_n^d$\;
\For{$i=1$ \KwTo $n$}{
    $\tau_i \leftarrow \tau\setminus\{x_i\}$\;
    $y_i \leftarrow \dfrac{1}{n-1}\sum_{x\in\tau_i}\dfrac{1}{V_n}\,
    \phi\!\left(\dfrac{x_i-x}{h_n}\right)$\;
}
$S \leftarrow \{(x_i,y_i)\}_{i=1}^n$\;
Sample $k$ auxiliary points $u_j\sim\mathrm{Unif}(D)$\;
\For{$j=1$ \KwTo $k$}{
    $\tilde y_j \leftarrow \dfrac{1}{n}\sum_{i=1}^{n}\dfrac{1}{V_n}\,\phi\!\left(\dfrac{u_j-x_i}{h_n}\right)$\;
}
$S\leftarrow S\cup\{(u_j,\tilde y_j)\}_{j=1}^{k}$\;
Train ANN by regression on $S$ (e.g.\ MSE loss)\;
$\hat p_\theta(\cdot)\leftarrow$ function computed by the trained ANN\;
\end{algorithm}

\subsection{Output Constraints and Non-Negativity}
A PNN is trained to regress density targets and is used as a non-negative density surrogate
$\hat p:\mathbb{R}^d\to\mathbb{R}_+$. Non-negativity is enforced by choosing the output as
\begin{equation}
    \hat p(x) = g(z(x)), \qquad g:\mathbb{R}\to\mathbb{R}_+,
\end{equation}
so that
\begin{equation}
    \hat p(x)\ge 0,\qquad \forall x\in\mathbb{R}^d.
\end{equation}
Typical choices are ReLU or a scaled sigmoid.

Three practical ways of enforcing non-negativity are considered:
\begin{itemize}
    \item \textbf{ReLU output:} $\hat p(x)=\max(0,z(x))$.
    \item \textbf{Scaled sigmoid:} $\hat p(x)=A\,\sigma(z(x))$ with $A>0$.
    \item \textbf{Log-parameterized mode:} the network outputs an unconstrained scalar $s_\theta(x)\in\mathbb{R}$
    interpreted as a log-density score.
\end{itemize}
In log-density mode, define
\begin{equation}
    \hat p_\theta(x) = \exp(s_\theta(x)).
\end{equation}
Training becomes regression on log-targets:
\begin{equation}
    \min_\theta\ \frac{1}{n}\sum_{i=1}^{n}\big(s_\theta(x_i)-\log(y_i+\varepsilon)\big)^2
    \equiv
    \min_\theta\ \frac{1}{n}\sum_{i=1}^{n}\big(\log \hat p_\theta(x_i)-\log(y_i+\varepsilon)\big)^2.
    \label{eq:log-density-loss}
\end{equation}
This transformation compresses the dynamic range of density values and turns multiplicative errors in
density space into additive errors in log-space, which is typically more numerically stable.

In particular, if an estimator has a multiplicative deviation $\hat p(x)=p(x)\,(1+\delta(x))$ with small
$|\delta(x)|$, then $\log \hat p(x)-\log p(x)\approx \delta(x)$, i.e., relative errors in density are mapped
to approximately additive residuals in log space. This is the main motivation for the default
    \texttt{log\_density} training mode: it simultaneously guarantees non-negativity via exponentiation and
improves numerical stability during optimization.

For the scaled-sigmoid output, the scale $A$ is chosen to match the typical magnitude of the Parzen
targets. A simple and effective heuristic is to set
\begin{equation}
    A = c\,\max_i y_i,\qquad c>1,
\end{equation}
so that the network is not artificially prevented from exceeding the maximum observed target during
approximation. In the reported experiments, $c=1.5$ is used as a conservative ``headroom'' factor.

The targets $y_i$ are constructed with a leave-one-out Parzen estimator, which avoids the self-kernel
contribution and yields an (asymptotically) unbiased estimate at sample locations:
\begin{equation}
    y_i=\frac{1}{n-1}\sum_{x\in\tau\setminus\{x_i\}}\frac{1}{V_n}
    \phi\!\left(\frac{x_i-x}{h_n}\right).
\end{equation}
Since targets are available only at the sample locations, the behavior between and outside samples is
determined by the inductive bias of the network architecture.

\subsection{Normalization over a Finite Domain}
A PNN does not enforce normalization:
\begin{equation}
    \int_{\mathbb{R}^d}\hat p(x)\,dx \neq 1 \quad \text{in general}.
\end{equation}
Therefore $\hat p$ is interpreted as an unnormalized density estimate. When a proper pdf is required,
normalization is performed on a fixed rectangle $D\subset\mathbb{R}^2$ discretized by a uniform grid
$\mathcal{G}=\{u_m\}_{m=1}^{M_D}$. Let $\Delta A=\Delta x\,\Delta y$ be the elementary cell area. The
normalizing constant is approximated by the Riemann sum
\begin{equation}
    Z \approx \sum_{u_m\in\mathcal{G}} \hat p_\theta(u_m)\,\Delta A,
    \qquad
    \hat p_{\mathrm{norm}}(x) = \frac{\hat p_\theta(x)}{Z}.
    \label{eq:riemann-normalization}
\end{equation}
The accuracy of Eq.~\eqref{eq:riemann-normalization} depends on both grid resolution and the choice of
domain $D$. If $D$ is too small, non-negligible probability mass may lie outside $D$, biasing the
normalization and any likelihood-style metric computed on $D$.

\section{Advanced Regularization Techniques}
\subsection{PDF Support and Boundary Penalty}
Standard activations are non-local basis functions; therefore fitting $\hat p(x_i)\approx y_i$ does not
imply $\hat p(x)\to 0$ away from the data and may generate heavy tails. To encourage compact support,
boundary constraints can be introduced. Let $X\subset\mathbb{R}^d$ denote the chosen support rectangle
(typically the same domain used for evaluation). Let $\xi$ be the diameter of $X$ and set $\delta=\alpha\xi$ with small
$\alpha\in(0,1)$. Define
\begin{equation}
    B_\delta=\{x\in\mathbb{R}^d\mid \mathrm{dist}(x,X)<\delta\}, \qquad
    \overline{B}_\delta=B_\delta\setminus X,
\end{equation}
with $\mathrm{dist}(x,X)=\inf_{y\in X}\|x-y\|$. Sample $\tilde x_j\sim\mathrm{Unif}(\overline{B}_\delta)$ and
add zero-density labels $S_\delta=\{(\tilde x_j,0)\}$. Training on $S\cup S_\delta$ can be interpreted as
minimizing the empirical regularized objective
\begin{equation}
    \min_\theta\ \frac{1}{n}\sum_{i=1}^n(\hat p_\theta(x_i)-y_i)^2
    +\lambda\,\frac{1}{k}\sum_{j=1}^k \hat p_\theta(\tilde x_j)^2,
\end{equation}
which penalizes probability mass near the boundary of $X$, stabilizes numerical normalization, and
mitigates spurious heavy tails.

\subsection{Internal Uniform Supervision}
The training set may be augmented with additional points sampled uniformly inside
the evaluation domain $D$. For each uniform point $u_j\sim\mathrm{Unif}(D)$ a Parzen/KDE target is computed
using the same kernel (with bandwidth $h_n$) and the pairs $\{(u_j, \hat p_{\mathrm{KDE}}(u_j))\}$ are added to
the regression dataset. This provides supervision away from sample locations and reduces extrapolation
artifacts when training only on pointwise leave-one-out targets.

\section{Experimental Setup and Model Selection}
\subsection{Dynamic Variables}
The experiments are organized as controlled sweeps over:
\begin{itemize}
    \item \textbf{Sample size $n$:} approximately 50 to 200 samples per Gaussian component.
    \item \textbf{PW bandwidth:} a grid of $h_1$ values spanning under- to over-smoothing.
    \item \textbf{PNN optimization:} Adam optimizer with learning-rate grid $\{5\times 10^{-3}\}$ (a singleton
    grid in the current experiments), trained for 3500 epochs per run, using full-batch updates.
    Unless otherwise stated, Adam hyperparameters use PyTorch defaults ($\beta_1=0.9$, $\beta_2=0.999$,
    $\varepsilon=10^{-8}$) and zero weight decay. Representative bandwidth values $h_1\in\{2,7,12,16\}$ are
    used for PNN training runs.
\end{itemize}

All density visualizations and grid-based metrics are computed on the fixed rectangular domain
$D=[-5,5]\times[-5,5]$ discretized by a uniform $100\times 100$ grid. The resulting grid spacing is
$\Delta x=\Delta y\approx 10/99\approx 0.101$. This resolution is sufficient for the reported mixtures at
the chosen bandwidths, but it is important to note that extremely narrow (low-variance) Gaussian
components would require a finer grid to capture steep gradients accurately in the Riemann-sum
normalization (Eq.~\eqref{eq:riemann-normalization}) and in grid-based $L_2$ error.

\subsection{Candidate Architectures}
MLPs with sigmoid hidden activations and non-negative outputs (ReLU or scaled sigmoid) are tested. The
architectures used in the sweeps include the following hidden-layer widths:
\begin{itemize}
    \item \textbf{[20]} and \textbf{[30,20]} with \textbf{output ReLU}.
    \item \textbf{[20]} and \textbf{[30,20]} with \textbf{output scaled-sigmoid} (with $A$ chosen
    automatically in the implementation).
\end{itemize}
This isolates the effect of capacity (width/depth) and output parameterization while keeping the target
generation mechanism fixed (leave-one-out Parzen targets).

From a modelling standpoint, increasing depth (e.g., moving from one to two hidden layers) can improve
representation of anisotropy and multimodality within a compact domain (pro), but may increase
overfitting risk and exacerbate spurious ``heavy tails'' outside high-density regions if not properly
regularized (con).

\subsection{Validation Criteria}
To select hyperparameters without using the ground truth, a held-out validation set
$\{x_j\}_{j=1}^{m}$ and compute the \textbf{validation negative log-likelihood (NLL)}:
\begin{equation}
    \mathrm{NLL}(\hat p;\{x_j\}_{j=1}^{m}) = -\frac{1}{m}\sum_{j=1}^{m}\log\big(\hat p(x_j)+\varepsilon\big).
\end{equation}
For PW/KDE, $\hat p$ is already normalized. For PNNs, normalization is performed on the finite domain $D$
(Section 3.3) before computing the NLL. This aligns with the principle of selecting the simplest model
that performs well on held-out data (an MDL-style viewpoint). Concretely, if two architectures achieve
similar validation NLL, the model with fewer parameters is preferred to reduce model complexity.
For the architectures considered here, an MLP with one hidden layer of width 20 has 81 parameters,
whereas the two-hidden-layer MLP with widths [30,20] has 731 parameters.

\section{Results Analysis and Discussion}
\subsection{Quantitative Performance}
Performance is evaluated on a fixed grid over a rectangle $D\subset\mathbb{R}^2$ that covers the target
mixtures. Accuracy is summarized via the $L_2$ error over the grid (a Riemann approximation of the
$L_2(D)$ norm):
\begin{equation}
    \|\hat p - p\|_{L^2(D)}^2 \approx \sum_{u_m\in\mathcal{G}}\big(\hat p(u_m)-p(u_m)\big)^2\,\Delta A.
\end{equation}
This allows direct comparison between PW and PNN as $n$ and $h_1$ vary.

Figures~\ref{fig:val-nll-sweep-m1}--\ref{fig:val-nll-sweep-m3} report the data-only cross-validation
criterion (validation NLL on held-out points) across bandwidths. Because PNNs are normalized on a finite
domain $D$ (Section 3.3), their NLL depends on $D$; the comparison is still meaningful as long as the
same $D$ is used consistently.

\subsubsection*{Findings}
The following relationships between \emph{inputs} (bandwidth and architecture choices) and \emph{outputs}
(validation NLL and oracle grid MSE) follow from standard bias--variance and approximation arguments, and
are confirmed by the saved sweeps:
\begin{itemize}
    \item \textbf{Bias--variance in $h_1$ is visible in validation NLL.}
    The best PNN validation NLL occurs at an \emph{intermediate} bandwidth in all mixtures
    (Mixture~1: best at $h_1=7$ with NLL $\approx 3.466$; Mixture~2: best at $h_1=7$ with NLL $\approx 3.359$;
    Mixture~3: best at $h_1=12$ with NLL $\approx 4.065$).
    This matches the expectation that small $h_1$ produces high-variance/undersmoothed estimates (risking
    near-zero density on validation points), while large $h_1$ increases bias (oversmoothing).

    \item \textbf{Depth consistently improves best oracle grid MSE in the sweep.}
    Across all three mixtures, the smallest oracle grid MSE observed among the saved runs is achieved by
    the deeper architecture \textbf{[30,20]} (Table~\ref{tab:oracle-best-mse}), consistent with greater
    representational flexibility on $D$.

    \item \textbf{The best output constraint is mixture-dependent.}
    Mixture~2 selects a \emph{scaled-sigmoid} output as best-by-validation (at $h_1=7$), while Mixture~3 selects a
    \emph{ReLU} output (at $h_1=12$). Scaled-sigmoid introduces an implicit cap (via $A=c\max_i y_i$), which can
    stabilize training for smoother targets, whereas ReLU remains unbounded and can represent sharper peaks.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/validation_nll_bandwidth_sweep_mixture1.jpeg}
\caption{Validation NLL (data-only cross-validation) as a function of bandwidth $h_1$ for Mixture 1.
Lower is better.}
\label{fig:val-nll-sweep-m1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/validation_nll_bandwidth_sweep_mixture2.jpeg}
\caption{Validation NLL (data-only cross-validation) as a function of bandwidth $h_1$ for Mixture 2.
Lower is better.}
\label{fig:val-nll-sweep-m2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/validation_nll_bandwidth_sweep_mixture3.jpeg}
\caption{Validation NLL (data-only cross-validation) as a function of bandwidth $h_1$ for Mixture 3.
Lower is better.}
\label{fig:val-nll-sweep-m3}
\end{figure}

\subsubsection*{Findings (PW / KDE)}
To explicitly study how Parzen Window (Gaussian KDE) behaves when both $n$ and $h_1$ vary, a PW-only
sweep was executed over many $(n,h_1)$ pairs and the oracle grid MSE was logged.
The resulting error surface (example in Fig.~\ref{fig:pw-errors-m3}) supports the following trends:
\begin{itemize}
    \item \textbf{The best achievable PW error improves with more samples.}
    When comparing the best grid MSE attainable at the largest sampled $n$ values versus the smallest,
    the mean best-MSE ratio is \emph{well below 1} (Mixture~1: $\approx 0.41$; Mixture~2: $\approx 0.33$;
    Mixture~3: $\approx 0.55$), consistent with variance reduction as $n$ increases.

    \item \textbf{The PW error surface exhibits an interior optimum in $h_1$.}
    For a representative mid-range $n$, both grid MSE and held-out NLL are minimized at intermediate
    bandwidths rather than at the smallest or largest tested $h_1$, matching the bias--variance
    trade-off for KDE.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/Parzen_errors_mixture3.jpeg}
\caption{PW/KDE oracle grid MSE over a sweep of sample size $n$ and base bandwidth $h_1$ (Mixture 3).
The surface illustrates the joint dependence on $n$ and $h_1$, with an interior optimal region in $h_1$
and improving best attainable error as $n$ increases.}
\label{fig:pw-errors-m3}
\end{figure}

For additional context (not usable for model selection), Table~\ref{tab:oracle-best-mse} summarizes the best
oracle grid MSE achieved among the saved training logs, where the MSE is computed against the known
mixture ground truth on the evaluation grid.

\begin{table}[H]
\centering
\begin{tabular}{l l r r}
\hline
Mixture & Best PNN configuration (logged) & $h_1$ & best grid MSE\\
\hline
1 & MLP [30,20], out ReLU (log-density training) & 2.0 & 2.68\,$\times 10^{-5}$\\
2 & MLP [30,20], out scaled-sigmoid (A=auto, log-density training) & 7.0 & 1.48\,$\times 10^{-5}$\\
3 & MLP [30,20], out ReLU (log-density training) & 12.0 & 8.84\,$\times 10^{-6}$\\
\hline
\end{tabular}
\caption{Best oracle grid MSE observed in the saved training logs for each mixture. These values use
ground truth and are included only for analysis and sanity-checking, not for selecting hyperparameters.}
\label{tab:oracle-best-mse}
\end{table}

\subsubsection*{Findings (PW vs PNN)}
Using the saved sweep artifacts where PW/KDE and PNN are evaluated on the \emph{same} held-out points and
the same domain $D$, the following comparison trends are validated:
\begin{itemize}
    \item \textbf{Best-by-NLL PNN is competitive with (and sometimes better than) PW/KDE.}
    In Mixture~1 the best PNN validation NLL is within $\approx 3\times 10^{-4}$ of the best KDE NLL;
    in Mixture~2 and Mixture~3, the best PNN NLL improves over KDE by about $0.066$ and $0.040$ respectively.

    \item \textbf{Best oracle grid MSE is comparable between PW/KDE and PNN.}
    The best PNN oracle grid MSE improves over the best KDE MSE in Mixtures~1--2 (ratios $\approx 0.70$ and
    $\approx 0.84$), while being essentially equal in Mixture~3 (ratio $\approx 1.01$).
\end{itemize}

\paragraph{Output activation behavior.}
The best-performing output parameterization differs across mixtures (Table~\ref{tab:oracle-best-mse}):
Mixture~2 achieves its best oracle grid MSE with a scaled-sigmoid output, while Mixture~3 achieves its
best with a ReLU output. A plausible explanation is related to peak sharpness and dynamic range.
The scaled-sigmoid output has an effective ``headroom'' determined by the scale $A=c\max_i y_i$ (with
$c=1.5$ in these experiments), which can constrain the maximum representable unnormalized density.
This can be beneficial when the target is moderately smooth (preventing excessive spikes), but may
limit approximation of very sharp/high peaks. Conversely, ReLU is unbounded and can represent taller
peaks when narrow components induce high local curvature, at the cost of requiring stronger
regularization to avoid spurious heavy tails.

\FloatBarrier
\subsection{Qualitative Analysis (Overlays)}
In addition to scalar metrics, 3D surface plots are inspected comparing (i) the true mixture density and
(ii) the estimated density. Overlay visualizations help diagnose oversmoothing (large bandwidth),
spurious bumps (small bandwidth), and missing/merged modes.

\subsubsection*{Findings}
The qualitative behavior in the overlays and learning curves is consistent with the following trends,
quantified from the saved training logs and sweep summaries:
\begin{itemize}
    \item \textbf{Smaller bandwidth slows optimization on sharp targets.}
    Define a convergence proxy as the number of epochs required to reach $90\%$ of the total EvalMSE
    improvement from initialization to the best achieved value. The median convergence epoch is higher
    for $h_1=2$ than for $h_1=16$ (approximately $1100$ vs $900$ epochs).

    \item \textbf{Smaller bandwidth increases late-training instability.}
    Using the standard deviation of EvalMSE over the last few logged checkpoints as a stability proxy,
    the median tail standard deviation is about $1.29\times 10^{-5}$ for $h_1=2$ versus
    $1.39\times 10^{-7}$ for $h_1=16$, indicating much noisier late-stage behavior at small bandwidth.

    \item \textbf{KDE undersmoothing can produce likelihood ``holes'' in complex mixtures.}
    With KDE at $h_1=2$, Mixture~3 attains a validation NLL of about $7.69$ while Mixture~1 attains about $4.21$
    (a gap of $\approx 3.48$), consistent with near-zero estimated density between modes when kernels are too narrow.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/overlays_h1_12p00_mixture1.jpeg}
\caption{Overlay comparison (KDE vs true and PNN vs true) for Mixture 1 at bandwidth $h_1=12$.}
\label{fig:overlays-h1-12-m1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/overlays_h1_12p00_mixture2.jpeg}
\caption{Overlay comparison (KDE vs true and PNN vs true) for Mixture 2 at bandwidth $h_1=12$.}
\label{fig:overlays-h1-12-m2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/overlays_h1_12p00_mixture3.jpeg}
\caption{Overlay comparison (KDE vs true and PNN vs true) for Mixture 3 at bandwidth $h_1=12$.}
\label{fig:overlays-h1-12-m3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/Parzen_overlay_mixture2.jpeg}
\caption{PW/KDE overlay example for Mixture 2 using a configuration selected by held-out NLL in the PW-only
sweep (true density overlaid with the KDE estimate). This visualization illustrates how the KDE surface
interpolates modes and how bandwidth controls the merge/split behavior of peaks.}
\label{fig:pw-overlay-m2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/learning_results_bandwidth_sweep_mixture1.jpeg}
\caption{PNN learning behavior across bandwidths for Mixture 1: training curves (top) and final oracle
grid MSE vs $h_1$ (bottom), compared to KDE.}
\label{fig:learning-sweeps-m1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/learning_results_bandwidth_sweep_mixture2.jpeg}
\caption{PNN learning behavior across bandwidths for Mixture 2: training curves (top) and final oracle
grid MSE vs $h_1$ (bottom), compared to KDE.}
\label{fig:learning-sweeps-m2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/learning_results_bandwidth_sweep_mixture3.jpeg}
\caption{PNN learning behavior across bandwidths for Mixture 3: training curves (top) and final oracle
grid MSE vs $h_1$ (bottom), compared to KDE.}
\label{fig:learning-sweeps-m3}
\end{figure}

\FloatBarrier
\subsection{Complexity Analysis}
Let $T$ be the number of query points where the density is evaluated. Parzen Window / KDE requires
evaluating $n$ kernels per query point, giving inference complexity $\mathcal{O}(nT)$. A PNN has a
potentially expensive training phase (including leave-one-out target generation), but after training the
test-time cost does not scale with $n$: evaluating a fixed network on $T$ points is $\mathcal{O}(WT)$,
where $W$ is the number of network parameters (often summarized as linear in $T$ for fixed architecture).

The main training-time cost specific to PNNs is the construction of leave-one-out targets: for each of
the $n$ samples, $n-1$ kernel evaluations are required, yielding
\begin{equation}
    \\text{target generation cost} = \\mathcal{O}(n^2).
\end{equation}
Training the network for $E$ epochs with $W$ parameters incurs an additional cost
$\mathcal{O}(n E W)$, so the total training complexity can be summarized as
\begin{equation}
    \mathcal{O}(n^2 + n E W).
\end{equation}

Figure~\ref{fig:loocv-complexity} empirically confirms the quadratic scaling of leave-one-out target
generation time with $n$.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/training_complexity_loocv.jpeg}
\caption{Empirical runtime of leave-one-out (LOO) target generation as a function of sample size $n$.
The measured curve is consistent with $\mathcal{O}(n^2)$ growth.}
\label{fig:loocv-complexity}
\end{figure}

To support the asymptotic analysis with empirical evidence, Table~\ref{tab:runtime} reports measured
wall-clock runtimes on this machine for a single evaluation of $T=10^4$ query points.

\begin{table}[H]
\centering
\begin{tabular}{lrrr}
\hline
Method & $n$ & $T$ & time (s)\\
\hline
PW (Gaussian KDE) & 2000 & $10^4$ & 0.923\\
PNN forward (fixed $W$) & -- & $10^4$ & 7.66\,$\times 10^{-4}$\\
\hline
\end{tabular}
\caption{Measured inference time for PW vs PNN on $T=10^4$ points (single run).}
\label{tab:runtime}
\end{table}

\subsubsection*{Findings}
Three relationships in this section are corroborated by the saved artifacts:
\begin{itemize}
    \item \textbf{Empirical LOOCV scaling is close to quadratic.}
    A log--log fit of target-generation runtime vs $n$ yields an exponent of approximately $1.87$, consistent
    with $\mathcal{O}(n^2)$ up to constants and cache effects.

    \item \textbf{Parameter counts differ by an order of magnitude.}
    The MLP [20] has $W=81$ parameters, while the MLP [30,20] has $W=731$. Since PNN inference is
    $\mathcal{O}(WT)$, this establishes an architecture-dependent scaling.

    \item \textbf{Measured inference times show a large speed gap in favor of PNN.}
    In Table~\ref{tab:runtime} (at $T=10^4$), the reported times imply a speedup of roughly
    $0.923\,/\,(7.66\times 10^{-4})\approx 1.2\times 10^3$ in favor of PNN forward evaluation.
\end{itemize}

\FloatBarrier
\subsection{Effect of Boundary Penalty}
When boundary regularization (Section 4.1) is enabled, the model is discouraged from placing probability
mass near/outside the domain boundary. This reduces heavy tails, stabilizes finite-domain normalization,
and \emph{may} improve validation NLL when an unregularized model assigns excessive mass to low-density regions.
The effect is assessed jointly via shape changes in overlay plots and validation NLL curves.

In addition to the qualitative plots, Table~\ref{tab:boundary-nll} reports validation NLL with and without
boundary penalty for the same configuration (selected by validation NLL) in each mixture. A decrease in
NLL indicates improved generalization on held-out data under finite-domain normalization.

\subsubsection*{Findings}
The saved boundary-penalty artifact reveals three practically relevant phenomena:
\begin{itemize}
    \item \textbf{Here the boundary penalty slightly worsens validation NLL.}
    For all three mixtures, Table~\ref{tab:boundary-nll} shows $\Delta$NLL $>0$ (about $+0.012$ to $+0.020$),
    indicating a small bias increase that is not compensated by improved generalization under the data-only NLL.

    \item \textbf{The penalty effect is larger when baseline boundary mass is larger.}
    In the saved demo, the baseline boundary mean is smallest in Mixture~1 ($\approx 8.6\times 10^{-3}$) and largest
    in Mixture~3 ($\approx 2.8\times 10^{-2}$), and correspondingly the absolute boundary-mean change is negligible in
    Mixture~1 but substantial in Mixture~3 (a decrease of about $3.9\times 10^{-3}$).

    \item \textbf{Boundary mean is not guaranteed to decrease under finite-domain normalization.}
    In Mixture~2, the boundary mean slightly increases when $\lambda$ is turned on, plausibly because the penalty
    modifies the unnormalized shape and the subsequent finite-domain renormalization can rescale the whole function.
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{l r r r}
\hline
Mixture & Val NLL ($\lambda=0$) & Val NLL ($\lambda=10^{-2}$) & $\Delta$ NLL\\
\hline
1 & 3.4765 & 3.4886 & +0.0121\\
2 & 3.3656 & 3.3852 & +0.0196\\
3 & 4.0509 & 4.0632 & +0.0123\\
\hline
\end{tabular}
\caption{Validation NLL with/without boundary penalty on the same domain $D$. Lower is better.
$\Delta$NLL denotes (with penalty) minus (without penalty).}
\label{tab:boundary-nll}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/boundary_penalty_comparison_mixture1.jpeg}
\caption{Boundary penalty comparison for Mixture 1, using the configuration selected by validation NLL.
KDE vs true (left), PNN with $\lambda=0$ (middle), PNN with $\lambda=10^{-2}$ (right).}
\label{fig:boundary-penalty-compare-m1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/boundary_penalty_comparison_mixture2.jpeg}
\caption{Boundary penalty comparison for Mixture 2, using the configuration selected by validation NLL.
KDE vs true (left), PNN with $\lambda=0$ (middle), PNN with $\lambda=10^{-2}$ (right).}
\label{fig:boundary-penalty-compare-m2}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{figures/boundary_penalty_comparison_mixture3.jpeg}
\caption{Boundary penalty comparison for Mixture 3, using the configuration selected by validation NLL.
KDE vs true (left), PNN with $\lambda=0$ (middle), PNN with $\lambda=10^{-2}$ (right).}
\label{fig:boundary-penalty-compare-m3}
\end{figure}

\FloatBarrier

\section{Conclusions}
The experiments compare a memory-based non-parametric estimator (PW/KDE) and a learned surrogate (PNN)
trained on Parzen-derived targets. The PNN can be viewed as a \emph{compressed} model that, with enough
capacity, can approximate complex multi-modal densities on a chosen compact domain.

This viewpoint is supported by the Universal Approximation Theorem: for a compact set $X\subset\mathbb{R}^d$
and any continuous target function $f\in C(X)$, a single-hidden-layer MLP with sigmoid activations can
approximate $f$ uniformly to arbitrary precision. In particular, for any $\varepsilon>0$ there exists a
network $f_\theta$ such that
\begin{equation}
    \sup_{x\in X}\,|f_\theta(x)-f(x)| < \varepsilon.
\end{equation}
Therefore, on a compact evaluation domain $D$, sufficiently wide sigmoid MLPs provide a principled function
class for approximating smooth density surfaces (after enforcing non-negativity and normalization).

The theoretical justification relies on approximating continuous functions on compact domains. In this
setting the target density $p$ is a Gaussian mixture, hence smooth ($C^\infty$) and strictly positive.
Moreover, the Parzen/KDE targets used to train the PNN are themselves smooth mixtures of Gaussians (a
finite sum of Gaussian kernels centered at samples), which makes the regression target particularly
well-suited to approximation by smooth MLPs. After enforcing non-negativity (e.g., via log-density
parameterization) and applying finite-domain normalization, the learned surrogate can be interpreted as
an approximate pdf on $D$.

Compared to PW/KDE, PNNs shift computation from test time to training time: once trained, inference is
efficient and does not grow with the number of samples. Overall performance depends on bandwidth choice,
architecture capacity, and regularization (boundary penalty and uniform supervision).

\paragraph{Summary of validated findings.}
\begin{itemize}
    \item \textbf{Bandwidth selection follows a bias--variance trade-off:} PNN validation NLL is minimized at an
    intermediate $h_1$ for all mixtures in the sweep (best at $h_1=7$ for Mixtures~1--2 and $h_1=12$ for Mixture~3).
    \item \textbf{Capacity matters for oracle accuracy:} the lowest oracle grid MSE observed in the saved runs is
    achieved by the deeper MLP [30,20] across all mixtures.
    \item \textbf{Output constraints interact with target sharpness:} Mixture~2 prefers scaled-sigmoid as best-by-NLL,
    while Mixture~3 prefers ReLU, consistent with an implicit cap vs unbounded peak representation.
    \item \textbf{Training dynamics degrade at small bandwidth:} $h_1=2$ slows convergence and increases late-stage
    EvalMSE variability relative to $h_1=16$.
    \item \textbf{KDE undersmoothing can break likelihood on complex mixtures:} Mixture~3 exhibits a much larger NLL at
    $h_1=2$ than Mixture~1, consistent with near-zero density regions between modes.
    \item \textbf{LOOCV target generation is empirically near-quadratic in $n$:} the measured log--log slope is $\approx 1.87$,
    supporting the $\mathcal{O}(n^2)$ analysis.
    \item \textbf{Boundary penalty is not automatically helpful:} in the saved artifact it slightly increases validation NLL,
    and its effect on boundary mass depends strongly on the baseline boundary leakage.
    \item \textbf{PW/KDE improves with more data and has an interior bandwidth optimum:} the PW-only $(n,h_1)$ sweep shows
    that best attainable grid MSE decreases as $n$ increases (best-MSE ratios $\approx 0.33$--$0.55$ from low to high $n$)
    and that both MSE and NLL are minimized at intermediate $h_1$.
    \item \textbf{PNN can match or outperform PW/KDE on the same selection metric:} best-by-NLL PNN is within $0.01$ of best
    KDE on all mixtures and improves over KDE on Mixtures~2--3 in the saved sweep.
\end{itemize}

\begin{thebibliography}{1}
\bibitem{cybenko1989}
G.~Cybenko, ``Approximation by superpositions of a sigmoidal function,'' \emph{Mathematics of Control,
Signals and Systems}, vol.~2, no.~4, pp.~303--314, 1989.
\end{thebibliography}

\end{document}